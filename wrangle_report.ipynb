{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs wrangling report\n",
    "\n",
    "Helen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Real-world data rarely comes clean. The goal of this project is to wrangle WeRateDogs Twitter data to create interesting and trustworthy analyses and visualizations.\n",
    "\n",
    "The required effort for my data wrangling in this project consists of:\n",
    "- Gathering data  \n",
    "- Assessing data\n",
    "- Cleaning data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather data from 3 different sources.\n",
    " \n",
    "\n",
    "- The tweet image predictions  :  This file (image_predictions.tsv) is hosted on Udacity's servers and should be downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "- Twitter API  : Query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source 1 : Enhanced WeRateDogs Twitter Archive**\n",
    "\n",
    "- Download file twitter_archive_enhanced.csv manually from link provided, given to Udacity from @WeRateDogs.\n",
    "\n",
    "- The WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets, but not everything. One column the archive does contain though: each tweet's text, which I used to extract rating, dog name, and dog \"stage\" (i.e. doggo, floofer, pupper, and puppo) to make this Twitter archive \"enhanced.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source 2 : Image Predictions File**\n",
    "- This file (image_predictions.tsv) is hosted on Udacity's servers and should be downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "  \n",
    "- One more cool thing: I ran every image in the WeRateDogs Twitter archive through a neural network that can classify breeds of dogs*. The results: a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source 3 : Additional Data via the Twitter API**\n",
    "\n",
    "- The recommended step to acquire this data should be query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt file\n",
    "\n",
    "- However, I can't set up a Twitter developer account using the steps recommended, therefore for this part I will gather the this piece of data without twitter account. \n",
    "\n",
    "- Back to the basic-ness of Twitter archives: retweet count and favorite count are two of the notable column omissions. Fortunately, this additional data can be gathered by anyone from Twitter's API. Well, \"anyone\" who has access to data for the 3000 most recent tweets, at least. But you, because you have the WeRateDogs Twitter archive and specifically the tweet IDs within it, can gather this data for all 5000+. And guess what? You're going to query Twitter's API to gather this valuable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Assess\n",
    "\n",
    "After gathering each of the above pieces of data, assess them visually and programmatically for quality and tidiness issues.\n",
    "\n",
    "Detect and document at least `eight (8) quality issues` and `two (2) tidiness issues`\n",
    "\n",
    "### Assessment Observations\n",
    "**Quality - Completeness, validity, accuracy, consistency (content issues)**\n",
    "\n",
    "\n",
    "`df_twitter_archive`\n",
    "\n",
    "1) wrong datatype for some columns eg : timestamp should be datetime instead of objec (in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id should be integers/strings instead of float.)\n",
    "\n",
    "2) There are 181 retweet entries. We only want  Use only original ratings tweets, not retweets.\n",
    "\n",
    "3) Rating denominator and numerators value are inconsistent or incorrect. \n",
    "\n",
    "  (However, rating numerators that are greater than the denominators does not need to be cleaned. This unique rating system is a big part of the popularity of WeRateDogs.)\n",
    "\n",
    "4) Wrong numerators and denominators are captured from text \n",
    "  - some are actually refer to date or other meaning 15/8 and 24/7.\n",
    "  - wrongly captured due to decimal and spaces \"11. 26/10\" was captured as \"26/10\"\n",
    "\n",
    "5) Tweets that has no image.\n",
    "  \n",
    "`df_image_predictions`\n",
    "\n",
    "6) Inconsistent capitalization on predicted dog names\n",
    "\n",
    "7) There are 324 rows non dog image (where p1, p2, and p3 are false)\n",
    "\n",
    "8) Duplicated jpg urls with different tweet ids\n",
    "\n",
    "9) 'None' string should be replaced with 'NaN'\n",
    "\n",
    "10) All 3 files contain different number of rows.\n",
    "\n",
    "**Tidiness**\n",
    "\n",
    "1)  2 columns storing rating information.\n",
    "\n",
    "2)  4 columns (doggo, floofer, pupper, puppo) to indicate dog stages.\n",
    "\n",
    "3)  All 3 files contain common tweet_id column, which can be used to join all three files as one dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will fix the quality and tidiness issues that we identified in step 2.\n",
    "\n",
    "Before cleaning start, the dataframe are copied to another new dataframes.\n",
    "\n",
    "Cleaning process involves three steps:\n",
    "- Define : convert assessments into defined cleaning tasks. These definitions also serve as an instruction list so others (in the future) can look at the work and reproduce it.\n",
    "- Code : convert those definitions to code and run that code.\n",
    "- Test : test dataset, visually or with code, to make sure cleaning operations worked.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, steps of assessing and cleaning data is repeated multiple times. Each time of repeating the steps make the data more meaningful to be analyized in later stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrangled data is stored in twitter_archive_master.csv.\n",
    "Then it is ready to be analyzed to draw useful insight from it, then to build visualization and reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
